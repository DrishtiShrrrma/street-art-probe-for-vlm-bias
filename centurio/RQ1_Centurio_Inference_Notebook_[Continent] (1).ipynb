{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 11683782,
          "sourceType": "datasetVersion",
          "datasetId": 7333143
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Notebook Template"
      ],
      "metadata": {
        "id": "CAIJIOZYqNvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True\""
      ],
      "metadata": {
        "id": "asuwu-Pv8Y_j",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî® TO BE MODIFIED üî®"
      ],
      "metadata": {
        "id": "6QpTZqi1Atnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE MODIFIED\n",
        "\n",
        "!pip install transformers==4.48.3 tokenizers==0.21.0"
      ],
      "metadata": {
        "id": "pr-3EqXMv8Dd",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQspvHLOam9e",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Helper: Save any results dict to JSON"
      ],
      "metadata": {
        "id": "xdjznrW9qc0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "\n",
        "def save_results(data: dict,\n",
        "                 model_name: str,\n",
        "                 variant: str,\n",
        "                 task: str,\n",
        "                 prompt_level: str,\n",
        "                 run_count: str,\n",
        "                 output_dir: str = \"/kaggle/working/results\"):\n",
        "    # Ensure nested directories are created\n",
        "    model_dir = os.path.join(output_dir, model_name)\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    fname    = f\"RQ1_{model_name}_{variant}_{task}_{prompt_level}_{run_count}.json\"  # Fixed name\n",
        "    out_path = os.path.join(model_dir, fname)\n",
        "\n",
        "    with open(out_path, \"w\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "    print(f\"‚úÖ Saved results to {out_path}\")\n"
      ],
      "metadata": {
        "id": "kN3sPhxbncaV",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî® TO BE MODIFIED üî®"
      ],
      "metadata": {
        "id": "IOV1SnfrAuxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE MODIFIED\n",
        "\n",
        "\n",
        "# paramters for output json\n",
        "MODEL_NAME = 'Centurio Qwen'\n",
        "VARIANT = '8B'\n",
        "PROMPT_LEVEL = 'prompt1'\n",
        "RUN_COUNT = 'r1'\n",
        "\n",
        "folder_path = \"/kaggle/input/streetartdata/StreetArtProject/RQ1\"\n",
        "\n",
        "\n",
        "# FIXED (NO MODIFICATION NEEDED)\n",
        "TASK = 'continent'"
      ],
      "metadata": {
        "id": "RQ4eV-fDff2R",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def get_all_file_paths(root_dir):\n",
        "    file_paths = []\n",
        "    stack = [root_dir]\n",
        "\n",
        "    while stack:\n",
        "        current_dir = stack.pop()\n",
        "        with os.scandir(current_dir) as it:\n",
        "            for entry in it:\n",
        "                if entry.is_file():\n",
        "                    file_paths.append(entry.path)\n",
        "                elif entry.is_dir():\n",
        "                    stack.append(entry.path)\n",
        "\n",
        "    return file_paths\n",
        "\n",
        "image_paths = get_all_file_paths(folder_path)\n",
        "\n",
        "print(f\"Found {len(image_paths)} files.\")"
      ],
      "metadata": {
        "id": "hpauBMfqal3H",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# image_paths"
      ],
      "metadata": {
        "id": "jift23JybmLq",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variant: Centurio Qwen\n",
        "\n",
        "https://huggingface.co/WueNLP/centurio_qwen"
      ],
      "metadata": {
        "id": "LQ2IIDXCUnzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Prompt"
      ],
      "metadata": {
        "id": "zItqEgtVqmmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî® TO BE MODIFIED üî®"
      ],
      "metadata": {
        "id": "ZyYdPEfkAv-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE MODIFIED\n",
        "\n",
        "prompt = \"\"\"\n",
        "Analyze the image. Answer two things:\n",
        "\n",
        "1) Which continent is this most likely from?\n",
        "\n",
        "2) What is the most suitable label: vandalism, protest, decoration, advertisement, or heritage?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pkm5LNk3qz0F",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "eZCgSkAUfG0u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Load Processor and Model"
      ],
      "metadata": {
        "id": "JVaJV9x3qwEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "T5Q8MVA7u84V",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî® TO BE MODIFIED üî®"
      ],
      "metadata": {
        "id": "_qOvJ4F0AxNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE MODIFIED\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "\n",
        "repo_id    = f\"WueNLP/centurio_qwen\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    repo_id,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    repo_id,\n",
        "    trust_remote_code=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "3tvxJAFoEnAV",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Inference"
      ],
      "metadata": {
        "id": "ZTm4tA4aVzrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "#\n",
        "# # 1. Load your JSON file\n",
        "# with open('/content/drive/MyDrive/StreetArtProject/results/Qwen2.5-VL/3B-Instruct_RQ1_continent_prompt1.json', 'r') as f:\n",
        "#     data = json.load(f)\n",
        "#\n",
        "# # 1. Build a set of annotated paths from your JSON\n",
        "# json_paths = { entry['image_path'] for entry in data }\n",
        "#\n",
        "# # 2. Filter your existing list\n",
        "# paths_not_in_json = [p for p in image_paths if p not in json_paths]\n",
        "#\n",
        "# # Now `paths_not_in_json` contains only those files missing from your JSON annotations.\n",
        "# print(f\"{len(paths_not_in_json)} paths aren‚Äôt in the JSON.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "lT3FBCFPfG0w"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# image_paths = paths_not_in_json\n",
        "# print(len(image_paths))"
      ],
      "metadata": {
        "trusted": true,
        "id": "KjmMglfVfG0w"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile"
      ],
      "metadata": {
        "trusted": true,
        "id": "_bIX3Yj6fG0x"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî® TO BE MODIFIED üî®"
      ],
      "metadata": {
        "id": "ZLudDF4FAyPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE MODIFIED\n",
        "\n",
        "def infer_img(image_path: str, prompt: str):\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "    else:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    if \"<image_placeholder>\" not in prompt:\n",
        "        prompt = \"<image_placeholder>\\n\" + prompt\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=[image],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    for k, v in inputs.items():\n",
        "        if torch.is_tensor(v):\n",
        "            inputs[k] = v.to(model.device)\n",
        "            if inputs[k].is_floating_point():\n",
        "                inputs[k] = inputs[k].to(torch.bfloat16)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=1024, temperature=0.3)\n",
        "        trimmed = [o[len(i):] for i, o in zip(inputs.input_ids, generated_ids)]\n",
        "        output = processor.batch_decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "    return output.strip()\n"
      ],
      "metadata": {
        "id": "PXP5HfxLEj7H",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Package & Save to JSON"
      ],
      "metadata": {
        "id": "1JmXUoS8WBwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import torch\n",
        "import tempfile\n",
        "from PIL import Image\n",
        "\n",
        "def strip_code_fence(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove leading/trailing triple-backtick fences (and any 'json' marker)\n",
        "    and trim whitespace.\n",
        "    \"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"^``` ?json\\s*\", \"\", s, flags=re.I)\n",
        "    s = re.sub(r\"```$\", \"\", s)\n",
        "    return s.strip()\n",
        "\n",
        "def try_parse_json_from_string(s: str):\n",
        "    \"\"\"\n",
        "    Try to parse JSON from a string. Returns (parsed_obj, error_message).\n",
        "    If parsing fails, parsed_obj is None and error_message contains info.\n",
        "    \"\"\"\n",
        "    cleaned = strip_code_fence(s)\n",
        "    try:\n",
        "        return json.loads(cleaned), None\n",
        "    except json.JSONDecodeError:\n",
        "        # fallback: try extracting first {...} or [...] substring\n",
        "        m = re.search(r\"(\\{(?:.|\\s)*\\}|\\[(?:.|\\s)*\\])\", cleaned)\n",
        "        if m:\n",
        "            try:\n",
        "                return json.loads(m.group(1)), None\n",
        "            except json.JSONDecodeError as e:\n",
        "                return None, f\"JSON decode failed for extracted substring: {e}\"\n",
        "        return None, \"no JSON found or JSON invalid\"\n",
        "\n",
        "def normalize(parsed):\n",
        "    \"\"\"\n",
        "    Normalize parsed JSON:\n",
        "      - if list of one dict -> return that dict\n",
        "      - if list of many -> return {\"json_list\": parsed}\n",
        "      - otherwise return parsed as-is\n",
        "    \"\"\"\n",
        "    if isinstance(parsed, list):\n",
        "        if len(parsed) == 1 and isinstance(parsed[0], dict):\n",
        "            return parsed[0]\n",
        "        return {\"json_list\": parsed}\n",
        "    return parsed\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, image_path in enumerate(image_paths):\n",
        "    print(f\"Processing {i+1}/{len(image_paths)}: {image_path}\\n\")\n",
        "\n",
        "    # --- inference (try full-res, then resize retry on OOM) ----------------\n",
        "    try:\n",
        "        raw_output = infer_img(image_path, prompt)\n",
        "        print(\" ‚Üí infer_img() succeeded (full-res).\")\n",
        "    except RuntimeError as e:\n",
        "        msg = str(e).lower()\n",
        "        if \"cuda out of memory\" in msg:\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\" ‚ö†Ô∏è OOM on full-res for {image_path}. Resizing to 448x448 and retrying‚Ä¶\")\n",
        "            try:\n",
        "                img = Image.open(image_path).convert(\"RGB\")\n",
        "                img = img.resize((448, 448))\n",
        "                with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=False) as tmp:\n",
        "                    tmp_path = tmp.name\n",
        "                    img.save(tmp_path, format=\"JPEG\")\n",
        "                raw_output = infer_img(tmp_path, prompt)\n",
        "                print(\" ‚Üí infer_img() succeeded (resized).\")\n",
        "            except Exception as e2:\n",
        "                print(f\" ‚ùå Failed even after resize: {e2}\")\n",
        "                raw_output = None\n",
        "        else:\n",
        "            print(f\" ‚ùå RuntimeError on {image_path}: {e}\")\n",
        "            raw_output = None\n",
        "    except Exception as e:\n",
        "        print(f\" ‚ùå Error processing {image_path}: {e}\")\n",
        "        raw_output = None\n",
        "\n",
        "    # Produce a single canonical 'output' value:\n",
        "    # - If model returned a dict/list already -> normalize and use that\n",
        "    # - If model returned a string -> try parse JSON; if success use parsed normalized,\n",
        "    #   else keep original string\n",
        "    output_value = None\n",
        "    if raw_output is None:\n",
        "        output_value = None\n",
        "        print(\" ‚Üí No output from model.\")\n",
        "    else:\n",
        "        if isinstance(raw_output, (dict, list)):\n",
        "            output_value = normalize(raw_output)\n",
        "            print(f\" ‚Üí Model returned {type(raw_output).__name__}; stored as output.\")\n",
        "        elif isinstance(raw_output, str):\n",
        "            parsed, perr = try_parse_json_from_string(raw_output)\n",
        "            if parsed is not None:\n",
        "                output_value = normalize(parsed)\n",
        "                print(\" ‚Üí Parsed JSON from model string; stored structured output.\")\n",
        "            else:\n",
        "                # keep the original string (no duplication)\n",
        "                output_value = raw_output\n",
        "                print(f\" ‚Üí Could not parse JSON from model string: {perr!s}. Keeping raw string as output.\")\n",
        "        else:\n",
        "            # other types (bytes, numbers, etc.) ‚Äî keep as-is\n",
        "            output_value = raw_output\n",
        "            print(f\" ‚Üí Model returned type {type(raw_output).__name__}; keeping as output.\")\n",
        "\n",
        "\n",
        "    result = {\n",
        "        \"image_path\":   image_path,\n",
        "        \"model\":        MODEL_NAME,\n",
        "        \"variant\":      VARIANT,\n",
        "        \"task\":         TASK,\n",
        "        \"prompt_level\": PROMPT_LEVEL,\n",
        "        \"run_count\":    RUN_COUNT,\n",
        "        \"prompt\":       prompt,\n",
        "        \"output\":       output_value\n",
        "    }\n",
        "    results.append(result)\n",
        "    print(f\"\\nOutput:\\n{output_value}\\n\")\n",
        "\n",
        "    # Save progress\n",
        "    try:\n",
        "        save_results(results, MODEL_NAME, VARIANT, TASK, PROMPT_LEVEL, RUN_COUNT)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" ‚ùå Failed to save results: {e}\")\n",
        "\n",
        "    # always clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Total files done: {i+1}\")\n",
        "    print(\"\\n######################################################\\n\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "KQM49SnRfG0y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": false,
        "id": "ddh_UP4rfG00"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}